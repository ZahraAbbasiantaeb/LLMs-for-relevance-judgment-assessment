{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "model_name = 'gpt3.5-zero-shot-paul-pool-temp0-'\n",
    "\n",
    "data_path = 'data/data.txt'\n",
    "path_output_pkl = 'LLM4Eval/outputs/'+model_name+'.pkl'\n",
    "output_text_path = 'LLM4Eval/outputs/'+model_name+'.txt'\n",
    "\n",
    "\n",
    "FROM_CHECKPOINT= False\n",
    "model_id = \"gpt-3.5-turbo-0125\"\n",
    "API_key = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the prompt per each turn\n",
    "\n",
    "def create_data_zero_shot(): \n",
    "    prompt = \"\"\"You are a search quality rater evaluating the relevance of web pages. \n",
    "    Given the persona of the user, user query, and a web page, you must provide a score on an integer scale of 0 to 4 to indicate to what extent the given document meets the information needs of the user. The scores have the following meanings:\n",
    "\n",
    "    0: fails to meet \n",
    "    1: slightly meets\n",
    "    2: moderately meets\n",
    "    3: highly meets\n",
    "    4: fully meets\n",
    "\n",
    "    User persona: {ptkb}\n",
    "    Query: {utterance}\n",
    "    Passage : {passage}\n",
    "    Score:\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    flattened_turn_pool_score = []\n",
    "\n",
    "    for line in lines:\n",
    "        turn_id, rewritten_utterance, response_txt,  passage_id, passage_text, score,  ptkb, _ =line.split('\\t')\n",
    "        x = {'turn_id': turn_id,\n",
    "        'doc_id': passage_id,\n",
    "        'passage': passage_text,\n",
    "        'score': score}\n",
    "        x['prompt'] = prompt.format(ptkb=ptkb, utterance=rewritten_utterance, passage=passage_text)\n",
    "        flattened_turn_pool_score.append(x)\n",
    "    \n",
    "    return flattened_turn_pool_score\n",
    "\n",
    "\n",
    "flattened_turn_pool_score =  create_data_zero_shot()\n",
    "\n",
    "if FROM_CHECKPOINT:\n",
    "    print('Loading from checkpoint')\n",
    "    with open(path_output_pkl, 'rb') as f:\n",
    "        flattened_turn_pool_score = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def chatgpt_conversation(conversation_Log):\n",
    "  response = client.chat.completions.create(\n",
    "      model = model_id,\n",
    "      messages= conversation_Log,\n",
    "      temperature= 0,\n",
    "      top_p= 1,\n",
    "      \n",
    "  )\n",
    "  \n",
    "  response= response.choices[0].message.content\n",
    "\n",
    "  return response\n",
    "\n",
    "def run_one_sample(prompt):\n",
    "\n",
    "    conversations = []\n",
    "    conversations.append({'role': 'user', 'content': prompt})\n",
    "    response = chatgpt_conversation(conversations)\n",
    "    score = response.strip()\n",
    "\n",
    "    # print()\n",
    "    # print('{0}: {1}\\n'.format(conversations[-1]['role'].strip(), conversations[-1]['content'].strip()))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=API_key,  # this is also the default, it can be omitted\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "index = 0\n",
    "\n",
    "for entry in flattened_turn_pool_score:\n",
    "    index += 1\n",
    "    print(index)\n",
    "\n",
    "    if 'gpt3.5-score' in entry:\n",
    "        print(entry['gpt3.5-score'])\n",
    "        print('&&&&&&&&&&&&&&')\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            pred_score = run_one_sample(entry['prompt'])\n",
    "            pred_score = pred_score.lower().replace('score:', '').strip()\n",
    "            entry['gpt3.5-score'] = pred_score\n",
    "            print(entry['gpt3.5-score'])\n",
    "            print('****************')\n",
    "            counter+=1\n",
    "        except:\n",
    "            print('unsuccessful')\n",
    "            entry['gpt3.5-score'] = 'NONE'\n",
    "\n",
    "    if index%100==99:\n",
    "        print('100 done')\n",
    "        with open(path_output_pkl, 'wb') as f:\n",
    "            pickle.dump(flattened_turn_pool_score, f)\n",
    "\n",
    "with open(path_output_pkl, 'wb') as f:\n",
    "    pickle.dump(flattened_turn_pool_score, f)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_output_pkl, 'wb') as f:\n",
    "    pickle.dump(flattened_turn_pool_score, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "X = 0\n",
    "lines = []\n",
    "\n",
    "for entry in flattened_turn_pool_score:\n",
    "    if ('gpt3.5-score' in entry):\n",
    "        numbers = re.findall(r'\\d+', entry['gpt3.5-score'].strip())\n",
    "        if len(numbers)==1:\n",
    "            score = numbers[0]\n",
    "        else:\n",
    "            if ('fails to meet' in entry['gpt3.5-score']):\n",
    "                score = '0'\n",
    "            elif ('slightly meets' in entry['gpt3.5-score']):\n",
    "                score = '1'\n",
    "            elif ('moderately meets' in entry['gpt3.5-score']):\n",
    "                score = '2'\n",
    "            elif ('highly meets' in entry['gpt3.5-score']):\n",
    "                score = '3'\n",
    "            elif ('fully meets' in entry['gpt3.5-score']):\n",
    "                score = '4'\n",
    "            else:\n",
    "                tmp = entry['gpt3.5-score'].strip().split('\\n')[0]\n",
    "                numbers = re.findall(r'\\d+',  tmp.strip()) \n",
    "                if len(numbers)==1:\n",
    "                    score = numbers[0]\n",
    "                else:                      \n",
    "                    score = entry['gpt3.5-score'].strip()\n",
    "                    print(score)\n",
    "                    X +=1\n",
    "\n",
    "        line = entry['turn_id'] + '\\t0\\t'+ entry['doc_id']+ '\\t'+ str(score) + '\\n'\n",
    "        lines.append(line)\n",
    "\n",
    "print(X)\n",
    "\n",
    "with open(output_text_path, 'w') as f:\n",
    "    f.writelines(lines)   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_text_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
