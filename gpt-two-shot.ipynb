{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/data.txt'\n",
    "model_name = 'gpt3.5-two-shot-pool-V2-temp0'\n",
    "path_output_pkl = 'outputs/'+model_name+'.pkl'\n",
    "output_text_path = 'outputs/'+model_name+'.txt'\n",
    "sample_passage_path = 'data/random_positive_negative_examples.txt'\n",
    "model_id = \"gpt-3.5-turbo-0125\"\n",
    "API_key = \"\"\n",
    "FROM_CHECKPOINT= True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading from checkpoint\n"
     ]
    }
   ],
   "source": [
    "# Create the prompt per each turn\n",
    "def create_data_two_shot():\n",
    "\n",
    "    prompt = \"\"\"You are a search quality rater evaluating the relevance of web pages. \n",
    "    Given the persona of the user, user query, and a web page, you must provide a score on an integer scale of 0 to 4 to indicate to what extent the given document meets the information needs of the user. The scores have the following meanings:\n",
    "\n",
    "    0: fails to meet \n",
    "    1: slightly meets\n",
    "    2: moderately meets\n",
    "    3: highly meets\n",
    "    4: fully meets\n",
    "\n",
    "    User persona: {ptkb}\n",
    "    Query: {utterance}\n",
    "\n",
    "    Passage 1: {pass_1}\n",
    "    Score: {score_1}\n",
    "\n",
    "    Passage 2: {pass_2}\n",
    "    Score: {score_2}\n",
    "\n",
    "    Passage 3: {passage}\n",
    "    Score:\n",
    "    Please only generate an int score between 0 to 4 to say to what extent the document 3 is relevant to the user question. Score lower than 2 means the document is not relevant.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    with open(sample_passage_path, 'r') as f:\n",
    "        sample_passages_data = f.readlines()\n",
    "    \n",
    "    sample_passage_turn = {}\n",
    "    \n",
    "    for line_data in sample_passages_data:\n",
    "        turn_id, pass_1, score_1, pass_2, score_2, _ = line_data.split('\\t')\n",
    "        y = {'pass_1': pass_1,\n",
    "             'pass_2': pass_2,\n",
    "             'score_1': score_1,\n",
    "             'score_2': score_2}\n",
    "        sample_passage_turn[turn_id] = y\n",
    "\n",
    "    with open(data_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    data_points = []\n",
    "\n",
    "    for line in lines:\n",
    "        turn_id, rewritten_utterance, response_txt,  passage_id, passage_text, score,  ptkb, _ =line.split('\\t')\n",
    "        x = {'turn_id': turn_id,\n",
    "        'doc_id': passage_id,\n",
    "        'passage': passage_text,\n",
    "        'score': score}\n",
    "\n",
    "        x['prompt'] = prompt.format(ptkb=ptkb, utterance=rewritten_utterance, passage=passage_text, \n",
    "                                    pass_1 = sample_passage_turn[turn_id]['pass_1'],\n",
    "                                    pass_2 = sample_passage_turn[turn_id]['pass_2'],\n",
    "                                    score_1 = sample_passage_turn[turn_id]['score_1'],\n",
    "                                    score_2 = sample_passage_turn[turn_id]['score_2'])\n",
    "        data_points.append(x)\n",
    "    \n",
    "    return data_points\n",
    "\n",
    "data_points = create_data_two_shot()\n",
    "\n",
    "if FROM_CHECKPOINT:\n",
    "    print('Loading from checkpoint')\n",
    "    with open(path_output_pkl, 'rb') as f:\n",
    "        data_points = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatgpt_conversation(conversation_Log):\n",
    "  response = client.chat.completions.create(\n",
    "      model = model_id,\n",
    "      messages= conversation_Log,\n",
    "      temperature= 0,\n",
    "      top_p= 1\n",
    "  )\n",
    "  \n",
    "  response= response.choices[0].message.content\n",
    "\n",
    "  return response\n",
    "\n",
    "def run_one_sample(prompt):\n",
    "\n",
    "    conversations = []\n",
    "    conversations.append({'role': 'user', 'content': prompt})\n",
    "    response = chatgpt_conversation(conversations)\n",
    "    score = response.strip()\n",
    "    \n",
    "    return score\n",
    "\n",
    "client = OpenAI(api_key=API_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "index = 0\n",
    "\n",
    "for entry in data_points:\n",
    "    index += 1\n",
    "    print(index)\n",
    "\n",
    "    if ('gpt3.5-score' in entry) and (not(entry['gpt3.5-score']=='NONE')):\n",
    "        print(entry['gpt3.5-score'])\n",
    "        print('&&&&&&&&&&&&&&')\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        try:\n",
    "            pred_score = run_one_sample(entry['prompt'])\n",
    "            pred_score = pred_score.lower().strip()\n",
    "            entry['gpt3.5-score'] = pred_score\n",
    "            print(entry['gpt3.5-score'])\n",
    "            print('****************')\n",
    "            counter+=1\n",
    "        except:\n",
    "            print('unsuccessful')\n",
    "            entry['gpt3.5-score'] = 'NONE'\n",
    "\n",
    "    if index%100==99:\n",
    "        print('100 done')\n",
    "        with open(path_output_pkl, 'wb') as f:\n",
    "            pickle.dump(data_points, f)\n",
    "\n",
    "\n",
    "with open(path_output_pkl, 'wb') as f:\n",
    "    pickle.dump(data_points, f)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "X = 0\n",
    "lines = []\n",
    "\n",
    "for entry in data_points:\n",
    "    if ('gpt3.5-score' in entry):\n",
    "        numbers = re.findall(r'\\d+', entry['gpt3.5-score'].strip())\n",
    "        \n",
    "        if len(numbers)==1:\n",
    "            score = int(numbers[0])\n",
    "\n",
    "        else:\n",
    "            tmp_lines = entry['gpt3.5-score'].strip().split('\\n')\n",
    "            tmp_boolean = True\n",
    "\n",
    "            for line in lines:\n",
    "                if 'passage 3 score:' in line:\n",
    "                    score = int(line.replace('passage 3 score:', '').strip())\n",
    "                    tmp_boolean = False\n",
    "                    break\n",
    "                \n",
    "                elif 'passage 3:' in line:\n",
    "                    score = int(line.replace('passage 3:', '').strip())\n",
    "                    tmp_boolean = False\n",
    "                    break\n",
    "            if tmp_boolean == False:\n",
    "                score = int(entry['gpt3.5-score'].strip())\n",
    "                X +=1\n",
    "\n",
    "        line = entry['turn_id'] + '\\t0\\t'+ entry['doc_id']+ '\\t'+ str(score) + '\\n'\n",
    "        lines.append(line)\n",
    "        \n",
    "print(X)\n",
    "\n",
    "with open(output_text_path, 'w') as f:\n",
    "    f.writelines(lines)  \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
